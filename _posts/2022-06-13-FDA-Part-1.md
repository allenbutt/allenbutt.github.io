---
layout: splash
title: "FDA Project Part 1 - Web Scraping"
subtitle: "Part 1  Web Scraping"
date: 2022-06-13 00:00:00 -0000
#background: 
---

# FDA Project - Part 1
## Background
Device malfunction reports are sent to the FDA from sources across the country. These reports are gathered and reviewed by experts in the FDA, and are also made available to the public. The goal of this project is three-fold:
1. Scrape the relevant data from the FDA website
2. Text analysis to determine key words from report descriptions
3. Visualizations to find interesting trends or areas needing further analysis
This post will focus on part 1 of this project.

The FDA keeps its publicly-available data in JSON format, with each different file stored in a downloadable zip folder on their site (https://open.fda.gov/data/downloads/). Each file inside its own ZIP folder has the same name as all others, making it a nightmare to manually download, unzip, rename, and merge the JSONs needed to make up the base dataset. Furthermore, any code written for analysis of this data would have to be fed a nicely-formatted file in order to be usable, putting undue stress on the end-user, who would have to perform all the above-mentioned steps without error. For these reasons, web-scraping and the automated assembly of these files according to the user's parameters became the first major priority.

## Selenium Web-Scraping
The FDA site in question was most certainly not made with web-scraping in mind (or any end-user experience, for that matter). Among various small obstacles like all the buttons on the page sharing the same name and a light-screen prompt to get past, there is one very large obstacle preventing basic web-scraping packages (like Beautiful Soup) from being able to retrieve the download links needed to progress the project: the list of Device Adverse Events downloads we need are hidden behind a button-prompt. Furthermore, the page does not load this button or anything behind it until the user scrolls their view to that area of the page. The relevant HTML code is hidden behind JAVA script, making it impossible to extract the information needed without first satisfying the page's scrolling and button-hitting requirement.

Fortunately, we can use the Selenium web-scraper to perform the necessary actions to populate the HTML code of the site without manual input. The script pulls up the FDA website, gets past the lightbox, scrolls to the button we need, and selects it. This allows us to extract a snippet of HTML with the list of relevant download links and bring it as a string into our code.

## Working with JSON Files and Zips
The HTML sample is turned into a list of URLS, each a download link to the Zip File with the JSON inside that we need. There is more data here than is immediately useful (there are thousands of device product codes and we only need a handful at a time), and the timeframe of the data goes back over 20 years. Instead of attempting to gather all the data and compile it into one very large dataset, you can specify the date range of data that is needed and a list of device codes to examine.

The relevant URLs for the downloads are located by filtering only the URLS that include years in the range specified by the user. Each URL for those years is followed, the ZIP file downloaded, the JSON file extracted, and then the data taken out and standardized into a dataframe before the JSON and the ZIP are deleted. Each JSON that is extracted is transformed into a dataframe and filtered according to relevant columns and Device codes, before being appended to the master dataframe. The end result can be saved as a CSV as a backup or for other use.

## Visualization Output
<iframe src="https://public.tableau.com/views/FDADeviceBarChartRaceDZE/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no&:embed=true"
width="1200" height="800"></iframe>