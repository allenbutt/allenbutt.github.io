---
layout: splash
title: "FDA Project Part 1 - Web Scraping"
subtitle: "Part 1  Web Scraping"
date: 2022-06-13 00:00:00 -0000
header:
  overlay_image: /assets/images/blue.jpg
  show_overlay_excerpt: false
#background: 
---

# FDA Project - Part 1
## Background
Device malfunction reports are sent to the FDA from across the country. These reports are gathered and reviewed by experts in the FDA, and are also made available to the public. The goal of this project is three-fold:
1. Scrape the relevant data from the FDA website
2. Text analysis to determine key words from report descriptions
3. Visualizations to find interesting trends or areas needing further analysis
This post will focus on part 1 of this project.

The FDA keeps its publicly-available data in JSON format, with each different file stored in a downloadable zip folder on their site (https://open.fda.gov/data/downloads/). Each file inside its own ZIP folder has the same name as all others, making it a nightmare to manually download, unzip, rename, and merge the JSONs needed to make up the base dataset. Furthermore, any code written for analysis of this data would have to be fed a nicely-formatted file in order to be usable, putting undue stress on the end-user, who would have to perform all the above-mentioned steps without error. For these reasons, web-scraping and the automated assembly of these files according to the user's parameters became the first major priority.

## Selenium Web-Scraping
The FDA site in question was most certainly not made with web-scraping in mind (or any end-user experience, for that matter). Among various small obstacles like all the buttons on the page sharing the same name and a light-screen prompt to get past, there is one very large obstacle preventing basic web-scraping packages (like Beautiful Soup) from being able to retrieve the download links needed to progress the project: the list of Device Adverse Events downloads we need are hidden behind a button-prompt. Furthermore, the page does not load this button or anything behind it until the user scrolls their view to that area of the page. The relevant HTML code is hidden behind JAVA script, making it impossible to extract the information needed without first satisfying the page's scrolling and button-hitting requirement.

Fortunately, we can use the Selenium web-scraper to perform the necessary actions to populate the HTML code of the site without manual input. The script pulls up the FDA website, gets past the lightbox, scrolls to the button we need, and selects it. This allows us to extract a snippet of HTML with the list of relevant download links and bring it as a string into our code.

```python
#The FDA website does not load the downloadable files unless you scroll to that area of the page first
#Web-Scraping involves using the Selenium webdriver to open the site with Chrome, navigate to the
    #needed area, and hit the correct buttons at the correct time

from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
import selenium.common.exceptions
from selenium import webdriver
import time

from selenium.webdriver.support.wait import WebDriverWait

options = webdriver.ChromeOptions()

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
driver.get("https://www.google.com")

driver.get(URL)
driver.maximize_window()

time.sleep(1)

button1 = driver.find_element(By.CLASS_NAME, "button.bg-primary.clr-white")
button1.click()

time.sleep(1)
 
element_link=WebDriverWait(driver, 10).until(EC.presence_of_element_located(
   (By.XPATH, '//*[@id="Medical Device Event"]')))

driver.execute_script("arguments[0].scrollIntoView(true)", element_link)

time.sleep(1)

button2 = driver.find_element(By.XPATH, '//*[@id="Medical Device Event"]/section/button')
button2.click()

time.sleep(1)

html = driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML")

driver.close()
```
 


## Working with JSON Files and Zips
The HTML sample is turned into a list of URLS, each a download link to the Zip File with the JSON inside that we need. There is more data here than is immediately useful (there are thousands of device product codes and we only need a handful at a time), and the timeframe of the data goes back over 20 years. Instead of attempting to gather all the data and compile it into one very large dataset, you can specify the date range of data that is needed and a list of device codes to examine.

The relevant URLs for the downloads are located by filtering only the URLS that include years in the range specified by the user. Each URL for those years is followed, the ZIP file downloaded, the JSON file extracted, and then the data taken out and standardized into a dataframe before the JSON and the ZIP are deleted. Each JSON that is extracted is transformed into a dataframe and filtered according to relevant columns and Device codes, before being appended to the master dataframe. The end result can be saved as a CSV as a backup or for other use.

```python
#Snip HTML to just the portion in question
pattern = '1991(.*?)<li id="Medical Device PMA">'
substring = re.search(pattern, html).group(1)
```


```python
#Itemize links into array
import lxml.html

url_list = lxml.html.fromstring(substring)
url_list = url_list.xpath('//a/@href')
```


```python
start_year = 2020
end_year = 2021

year_list = list(range(start_year, end_year+1))

index_to_download = []

for meh in year_list:
    for bleh in range(0, len(url_list)):
        if str(meh) in url_list[bleh]:
            index_to_download.append(url_list.index(url_list[bleh]))

index_count = len(index_to_download)
```


```python
#Follow links in array to download/process ZIPs

#Specify Download Path
path = 'C:/Users/Allen/Documents/FDA'
#pathinverse = 'C:\Users\Allen\Documents\FDA'
import requests, zipfile
from io import BytesIO

#Filter Data by Project Code, use "All" to include all data
pcode = ["KDK","KDL","KDN","LLJ","MFU","MFV","MSB","PGR","PHO"]
#pcode = ["All"]

#Run loop, opening JSONs
loopnumber = 0
datamain = ""
for snuh in index_to_download:
    print('Download ' + (str(loopnumber+1)) + " of " + (str(index_count)) + " started ")
    url = url_list[snuh]
    import requests, zipfile
    req = requests.get(url)
    print('Download ' + (str(loopnumber+1))+ " completed ")
    zipfile = zipfile.ZipFile(BytesIO(req.content))
    #filename = "FDA" + str(snuh+1)
    filename = "FDAdata.json"
    for i, f in enumerate(zipfile.filelist):
        f.filename = filename.format(i)
        zipfile.extract(f)
    print('File ' + (str(loopnumber+1))+ ' extracted')
    data = json.load(open(r'C:\Users\Allen\Documents\FDA\FDAdata.json'))
    data = data["results"]
    datamain = data
    if loopnumber == 0:
        print('Creating Dataframe with JSON ' + (str(loopnumber+1)))
        dfmain = pd.json_normalize(data,
                  record_path = "device",
                  meta = ["report_number","report_source_code","date_received","event_type","type_of_report","mdr_text"],
                  record_prefix = "_",
                  errors = "ignore")
        if pcode[0] != "All":
            dfmain = dfmain[dfmain._device_report_product_code.isin(pcode)]
        print('Dataframe Created')
    else:
        print('Appending Dataframe with JSON ' + (str(loopnumber+1)))
        dfnew = pd.json_normalize(data,
                  record_path = "device",
                  meta = ["report_number","report_source_code","date_received","event_type","type_of_report","mdr_text"],
                  record_prefix = "_",
                  errors = "ignore")
        if pcode[0] != "All":
            dfnew = dfnew[dfnew._device_report_product_code.isin(pcode)]
        dfmain = pd.concat([dfmain, dfnew])
        #dfmain.append(dfnew)
        print('JSON ' + (str(loopnumber+1)) + ' appended')
        
    os.remove(path + "/" + filename)
    loopnumber = loopnumber + 1
print("Dataframe ready")
```

    ...
    Appending Dataframe with JSON 36
    JSON 36 appended
    Download 37 of 38 started 
    Download 37 completed 
    File 37 extracted
    Appending Dataframe with JSON 37
    JSON 37 appended
    Download 38 of 38 started 
    Download 38 completed 
    File 38 extracted
    Appending Dataframe with JSON 38
    JSON 38 appended
    Dataframe ready

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_device_event_key</th>
      <th>_implant_flag</th>
      <th>_date_removed_flag</th>
      <th>_device_sequence_number</th>
      <th>_date_received</th>
      <th>_brand_name</th>
      <th>_generic_name</th>
      <th>_manufacturer_d_name</th>
      <th>_manufacturer_d_address_1</th>
      <th>_manufacturer_d_address_2</th>
      <th>...</th>
      <th>_openfda.device_class</th>
      <th>_expiration_date_of_device</th>
      <th>_openfda.registration_number</th>
      <th>_openfda.fei_number</th>
      <th>report_number</th>
      <th>report_source_code</th>
      <th>date_received</th>
      <th>event_type</th>
      <th>type_of_report</th>
      <th>mdr_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>98596</th>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>20200206</td>
      <td>LIFEPORT KIDNEY TRANSPORTER SYSTEM</td>
      <td>PERFUSION CIRCUIT</td>
      <td>ORGAN RECOVERY SYSTEMS, INC.</td>
      <td>1 PIERCE PLACE</td>
      <td>SUITE 475W</td>
      <td>...</td>
      <td>2</td>
      <td>20210318</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3004068499-2020-00003</td>
      <td>Manufacturer report</td>
      <td>20200206</td>
      <td>Malfunction</td>
      <td>[Initial submission, Followup]</td>
      <td>[{'mdr_text_key': '221303083', 'text_type_code...</td>
    </tr>
    <tr>
      <th>17151</th>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>20200227</td>
      <td>LIFEPORT KIDNEY TRANSPORTER SYSTEM</td>
      <td>PERFUSION CIRCUIT</td>
      <td>ORGAN RECOVERY SYSTEMS, INC.</td>
      <td>1 PIERCE PLACE</td>
      <td>SUITE 475W</td>
      <td>...</td>
      <td>2</td>
      <td>20210318</td>
      <td>[3004068499]</td>
      <td>[3004068499]</td>
      <td>3004068499-2020-00004</td>
      <td>Manufacturer report</td>
      <td>20200227</td>
      <td>Malfunction</td>
      <td>[Initial submission]</td>
      <td>[{'mdr_text_key': '224587763', 'text_type_code...</td>
    </tr>
    <tr>
      <th>42763</th>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>20200206</td>
      <td>LIFEPORT KIDNEY TRANSPORTER SYSTEM</td>
      <td>PERFUSION CIRCUIT</td>
      <td>ORGAN RECOVERY SYSTEMS, INC.</td>
      <td>1 PIERCE PLACE</td>
      <td>SUITE 475W</td>
      <td>...</td>
      <td>2</td>
      <td>20210424</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3004068499-2020-00001</td>
      <td>Manufacturer report</td>
      <td>20200206</td>
      <td>Malfunction</td>
      <td>[Initial submission, Followup]</td>
      <td>[{'mdr_text_key': '196742172', 'text_type_code...</td>
    </tr>
    <tr>
      <th>70387</th>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>20200206</td>
      <td>LIFEPORT KIDNEY TRANSPORTER SYSTEM</td>
      <td>PERFUSION CIRCUIT</td>
      <td>ORGAN RECOVERY SYSTEMS, INC.</td>
      <td>1 PIERCE PLACE</td>
      <td>SUITE 475W</td>
      <td>...</td>
      <td>2</td>
      <td>20210318</td>
      <td>[3004068499]</td>
      <td>[3004068499]</td>
      <td>3004068499-2020-00002</td>
      <td>Manufacturer report</td>
      <td>20200206</td>
      <td>Malfunction</td>
      <td>[Initial submission, Followup]</td>
      <td>[{'mdr_text_key': '221303162', 'text_type_code...</td>
    </tr>
    <tr>
      <th>20329</th>
      <td></td>
      <td></td>
      <td></td>
      <td>1</td>
      <td>20200320</td>
      <td>LIFEPORT KIDNEY TRANSPORTER SYSTEM</td>
      <td>PERFUSION CIRCUIT</td>
      <td>ORGAN RECOVERY SYSTEMS, INC.</td>
      <td>1 PIERCE PLACE</td>
      <td>SUITE 475W</td>
      <td>...</td>
      <td>2</td>
      <td>20211029</td>
      <td>[3004068499]</td>
      <td>[3004068499]</td>
      <td>3004068499-2020-00005</td>
      <td>Manufacturer report</td>
      <td>20200320</td>
      <td>Malfunction</td>
      <td>[Initial submission]</td>
      <td>[{'mdr_text_key': '222081101', 'text_type_code...</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 40 columns</p>
</div>

The above table has more information than necessary, and requires further transformation. The important part of the text is also nested in the MDR Text field, which creates its own additional challenges. These will be explored in part 2 of this project.